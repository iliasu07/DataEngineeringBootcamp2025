# Data Engineering Zoomcamp 2025 - DataTalks.Club


## Overview
This repository contains my journey through the **Data Engineering Zoomcamp** offered by [DataTalks.Club](https://github.com/DataTalksClub/data-engineering-zoomcamp). The course provides an in-depth understanding of data engineering concepts, covering areas such as data ingestion, warehousing, orchestration, batch and streaming processing, and cloud deployments.

In addition to completing the course, I extended my learning by implementing various data engineering solutions using **Google Cloud Platform (GCP)** and leveraging **AWS services** to compare and enhance scalability, cost efficiency, and performance.

## Course Topics Covered
### **Week 1: Introduction & Setup**
- Introduction to data engineering
- Setting up local and cloud environments (GCP & AWS)
- Docker and PostgreSQL setup

### **Week 2: Data Ingestion**
- Extracting and loading data from APIs
- Using **Google Cloud Storage (GCS)** and **BigQuery** for storage and analytics
- Implementing a similar pipeline using **AWS S3** and **AWS Glue**

### **Week 3: Data Warehouse & Transformation**
- **BigQuery** architecture and optimizations
- Data modeling and partitioning
- Implementing the equivalent workflow using **AWS Redshift**

### **Week 4: Analytics Engineering**
- Introduction to **dbt (Data Build Tool)** for transformations
- Creating data models and incremental transformations
- Migrating **dbt workflows** from GCP to AWS using **dbt with Redshift**

### **Week 5: Batch Processing**
- Working with **Apache Spark** and **Dataflow (GCP)**
- Implementing similar batch processing using **AWS Glue and EMR**
- Optimizing Spark jobs for cost-effective execution

### **Week 6: Streaming Processing**
- Setting up **Kafka and Pub/Sub** for real-time data processing
- Implementing event-driven pipelines using **AWS Kinesis and Lambda**
- Comparing latency and scalability across platforms

### **Week 7: Orchestration & Automation**
- **Apache Airflow** for workflow orchestration
- Deploying DAGs on **Google Composer** and **MWAA (Managed Workflows for Apache Airflow) on AWS**
- Automating ETL pipelines for end-to-end data flow

### **Week 8: Data Governance & Security**
- Implementing **IAM roles and access policies** in GCP and AWS
- Using **AWS Lake Formation** for access control
- Ensuring **GDPR compliance** and encryption best practices

## Key Learnings & AWS Integration
While following the **Data Engineering Zoomcamp** on GCP, I researched and implemented equivalent solutions using AWS services:
- **Storage:** GCS â†’ S3, BigQuery â†’ Redshift
- **ETL Pipelines:** Dataflow â†’ Glue, Cloud Functions â†’ AWS Lambda
- **Streaming:** Pub/Sub â†’ Kinesis, Kafka setup across both cloud providers
- **Orchestration:** Cloud Composer â†’ MWAA (Managed Apache Airflow on AWS)
- **Security & Governance:** IAM policies, encryption strategies, and best practices

## Tools & Technologies Used
- **Programming Languages:** Python, SQL
- **Cloud Platforms:** Google Cloud Platform (GCP), Amazon Web Services (AWS)
- **Databases & Warehouses:** BigQuery, Redshift, PostgreSQL
- **Orchestration & Workflow Automation:** Apache Airflow, dbt
- **Processing Frameworks:** Spark, Dataflow, Glue, EMR
- **Streaming Technologies:** Kafka, Pub/Sub, Kinesis
- **Infrastructure & Deployment:** Terraform, Docker, Kubernetes (GKE & EKS)

## Conclusion
This project was an invaluable experience in understanding real-world data engineering challenges. By comparing **GCP and AWS solutions**, I gained insights into cost trade-offs, performance optimizations, and scalability considerations across cloud providers.

Feel free to explore the repository, and if you have any questions, reach out via LinkedIn or GitHub discussions!

---

### Connect with Me
ðŸ“Œ **GitHub:** (https://github.com/iliasu07)  
ðŸ“Œ **LinkedIn:** (http://www.linkedin.com/in/iliasu-salaudeen-55988b222)


